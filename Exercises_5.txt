#5 

I would ask for the API documentation or the data sources for the world daily weather that the function is extracting, transforming, and loading data from. Prior to the data validation process, looking at the documents will help to ensure the data quality is high, as in accurate, consistent, and complete. It will also help to set minimum data quality requirements and transformation logic; so like, how was the data formatted in the returned data dictionary from the source, or how was the key 'precipitation_type' categorized into the values none/’rain’/’snow’/’mixed’ from the source data. I could also check to see if the documentation was unstructured or converted to structured data before data loading. All this will help make test automation simpler for comprehensive database testing. 

I would perform functional testing on the daily weather program to test the quality and accuracy of the data, and that there are no errors while loading. In this process, I would implement unit tests to test individual parts. For example, each question would be a unit test to test the function is working:
1) Data Loading Process Validation: Was the input weather data compared with the source weather data to make sure it matches? Was the desired data pulled (date, minimum temperature, maximum temperature, average temperature, precipitation type, and precipitation amount), with the right data type requirements (datetime, float, float, float, string or none, and float, respectively)? Was data extracted and loaded into the location correctly from the source? Was data split into different nodes? 
2) Map Reduce Process Validation: Did the data process complete and create the output to be sent to the weather program interface? Is the business logic validated by running on each standalone node and then against multiple nodes? Did the map reduce process complete, with key-value pairs generated for the requested zipcode's weather data dictionary? Is the data aggregated and segregated? Is the output file correct and compared with source data? Is the output file in the correct format per data requirements?
3) Data Warehouse Process Validation: Were the transformation rules applied correctly? Was the data loaded into the warehouse correctly? Was the target data compared with the system data to check for data corruption? Was data integrity intact, as in consistent in all places, and extracted completely? 

Other than unit tests, I would also implement integration tests to ensure the interface and database are working well together. Non-functional testing, such as failover testing by testing each node would also be helpful. Moreover, performance testing would test in isolation how quickly each data processing step takes to complete, how much memory is used in a transaction, and how quickly the system can consume data from the weather data source when there is an increase in user load. The purpose of this is to ascertain effective scalability and short load time for the program. Lastly, I would also implement end to end testing to test that the data from the source is making it all the way through the program and outputting the correct weather for the zipcode and date provided in the function.